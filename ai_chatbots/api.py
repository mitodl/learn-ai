"""AI-specific functions for ai_chatbots."""

import json
import logging
from typing import Any, Optional, Union, cast
from uuid import UUID, uuid4

import litellm
from channels.db import database_sync_to_async
from django.conf import settings
from django.db import transaction
from langchain_core.language_models import LanguageModelLike
from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    BaseMessage,
    HumanMessage,
    RemoveMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.messages.utils import count_tokens_approximately
from langchain_core.outputs import LLMResult
from langchain_core.prompt_values import ChatPromptValue
from langchain_core.prompts import ChatPromptTemplate
from langgraph.utils.runnable import RunnableCallable
from langmem.short_term import RunningSummary
from langmem.short_term.summarization import (
    DEFAULT_EXISTING_SUMMARY_PROMPT,
    DEFAULT_FINAL_SUMMARY_PROMPT,
    DEFAULT_INITIAL_SUMMARY_PROMPT,
    SummarizationResult,
    TokenCounter,
)
from langsmith import Client as LangsmithClient
from posthog.ai.langchain import CallbackHandler
from pydantic import BaseModel
from typing_extensions import TypedDict

from ai_chatbots.constants import WRITES_MAPPING
from ai_chatbots.models import DjangoCheckpoint, TutorBotOutput, UserChatSession
from main.utils import now_in_utc

log = logging.getLogger(__name__)


def get_search_tool_metadata(thread_id: str, latest_state: TypedDict) -> str:
    """
    Return the metadata for a bot search tool.
    """
    tool_messages = (
        []
        if not latest_state
        else [
            t
            for t in latest_state.values.get("messages", [])
            if t and t.__class__ == ToolMessage
        ]
    )
    if tool_messages:
        msg_content = tool_messages[-1].content
        try:
            content = json.loads(msg_content or "{}")
            return {
                "metadata": {
                    "search_url": content.get("metadata", {}).get("search_url"),
                    "search_parameters": content.get("metadata", {}).get(
                        "parameters", []
                    ),
                    "search_results": content.get("results", []),
                    "citation_sources": content.get("citation_sources", []),
                    "thread_id": thread_id,
                }
            }
        except json.JSONDecodeError:
            log.exception(
                "Error parsing tool metadata, not valid JSON: %s", msg_content
            )
            return {"error": "Error parsing tool metadata", "content": msg_content}
    else:
        return {}


def summarize_messages(  # noqa: PLR0912, PLR0913, PLR0915, C901
    messages: list[AnyMessage],
    *,
    running_summary: RunningSummary | None,
    model: LanguageModelLike,
    max_tokens: int,
    max_tokens_before_summary: int | None = None,
    max_summary_tokens: int = 256,
    token_counter: TokenCounter = count_tokens_approximately,
    initial_summary_prompt: ChatPromptTemplate = DEFAULT_INITIAL_SUMMARY_PROMPT,
    existing_summary_prompt: ChatPromptTemplate = DEFAULT_EXISTING_SUMMARY_PROMPT,
    final_prompt: ChatPromptTemplate = DEFAULT_FINAL_SUMMARY_PROMPT,
) -> SummarizationResult:
    """
    Summarize and replace messages when they exceed a token limit.

    This function is based on the `langmem.short_term.summarization.summarize_messages`
    function with only minor changes (it omits the strict removal of earlier messages to
    summarize depending on the max token count).

    This function processes the messages from oldest to newest: once the cumulative
    number of message  tokens reaches `max_tokens_before_summary`, all messages within
    `max_tokens_before_summary` are summarized (excluding the system message, if any)
    and replaced with a new summary message. The resulting list of messages is
    [summary_message] + remaining_messages.

    Args:
        messages: The list of messages to process.
        running_summary: Optional running summary w/info about previous summarization.
        If provided:
            - only messages that were **not** previously summarized will be processed
            - if no new summary is generated, the running summary will be added to the
                returned messages
            - if a new summary needs to be generated, it is generated by incorporating
                the existing summary value from the running summary
        model: The language model to use for generating summaries.
        max_tokens: Maximum number of tokens to return in the final output. Will be
            enforced only after summarization.
        max_tokens_before_summary: Maximum number of tokens to accumulate before
            triggering summarization. Defaults to the same value as `max_tokens` if not
            provided. This allows fitting more tokens into the summarization LLM,
            if needed.
        max_summary_tokens: Maximum number of tokens to budget for the summary.
        token_counter: Function to count tokens in a message. Defaults to approximate
            counting.For more accurate counts use `model.get_num_tokens_from_messages`.
        initial_summary_prompt: Prompt template for generating the first summary.
        existing_summary_prompt: Prompt template for updating a running summary.
        final_prompt: Prompt template w/summary & remaining messages before returning.

    Returns:
        SummarizationResult object containing the updated messages and running summary.
            - messages: list of updated messages ready to be input to the LLM
            - running_summary: RunningSummary object
                - summary: text of the latest summary
                - summarized_message_ids: set of previously summarized message IDs
                - last_summarized_message_id: ID of the last message that was summarized
    """
    # Set max_tokens_before_summary to max_tokens if not provided
    if max_tokens_before_summary is None:
        max_tokens_before_summary = max_tokens

    max_tokens_to_summarize = max_tokens
    # Adjust the remaining token budget to account for the summary to be added
    max_remaining_tokens = max_tokens - max_summary_tokens
    # First handle system message if present
    if messages and isinstance(messages[0], SystemMessage):
        existing_system_message = messages[0]
        # remove the system message from the list of messages to summarize
        messages = messages[1:]
        # adjust remaining token budget for the system msg to be re-added
        max_remaining_tokens -= token_counter([existing_system_message])
    else:
        existing_system_message = None

    # Summarize only when last message is a human message
    if not messages or (messages and not isinstance(messages[-1], HumanMessage)):
        return SummarizationResult(
            running_summary=running_summary,
            messages=(
                messages
                if existing_system_message is None
                else [existing_system_message, *messages]
            ),
        )

    # Get previously summarized messages, if any
    summarized_message_ids = set()
    total_summarized_messages = 0
    existing_summary = running_summary
    if running_summary:
        summarized_message_ids = running_summary.summarized_message_ids
        # Adjust the summarization token budget to account for the previous summary
        max_tokens_to_summarize -= token_counter(
            [SystemMessage(content=running_summary.summary, id=str(uuid4()))]
        )
        # If we have an existing running summary, find how many messages have been
        # summarized so far based on the last summarized message ID.
        for i, message in enumerate(messages):
            if message.id == running_summary.last_summarized_message_id:
                total_summarized_messages = i + 1
                break

    # We will use this to ensure that the total number of resulting tokens
    # will fit into max_tokens window.
    total_n_tokens = token_counter(messages[total_summarized_messages:])

    # Go through messages to count tokens and find cutoff point
    n_tokens = 0
    idx = max(0, total_summarized_messages - 1)
    # map tool call IDs to their corresponding tool messages
    tool_call_id_to_tool_message: dict[str, ToolMessage] = {}
    should_summarize = False
    for i in range(total_summarized_messages, len(messages)):
        message = messages[i]
        if message.id is None:
            err = f"Messages are required to have ID field: {message}"
            raise ValueError(err)

        if message.id in summarized_message_ids:
            err = f"Message with ID {message.id} has already been summarized."
            raise ValueError(err)

        # Store tool messages by their tool_call_id for later reference
        if isinstance(message, ToolMessage) and message.tool_call_id:
            tool_call_id_to_tool_message[message.tool_call_id] = message

        n_tokens += token_counter([message])

        # Check if we've reached max_tokens_to_summarize and
        # final message is a valid type to end summarization on
        # (not a tool message or AI tool)
        if (
            n_tokens >= max_tokens_before_summary
            and total_n_tokens - n_tokens <= max_remaining_tokens
            and not should_summarize
            and not isinstance(message, ToolMessage)
            and (not isinstance(message, AIMessage) or not message.tool_calls)
        ):
            should_summarize = True
            idx = i

    if not should_summarize:
        messages_to_summarize = []
    else:
        messages_to_summarize = messages[total_summarized_messages : idx + 1]

    if messages_to_summarize:
        if running_summary:
            summary_messages = cast(
                ChatPromptValue,
                existing_summary_prompt.invoke(
                    {
                        "messages": messages_to_summarize,
                        "existing_summary": running_summary.summary,
                    }
                ),
            )
        else:
            summary_messages = cast(
                ChatPromptValue,
                initial_summary_prompt.invoke({"messages": messages_to_summarize}),
            )
        log.debug("messages to summarize: %s", messages_to_summarize)
        summary_response = model.invoke(summary_messages.messages)
        log.debug("Summarization response: %s", summary_response.content)
        summarized_message_ids = summarized_message_ids | {
            message.id for message in messages_to_summarize
        }
        total_summarized_messages += len(messages_to_summarize)
        running_summary = RunningSummary(
            summary=summary_response.content,
            summarized_message_ids=summarized_message_ids,
            last_summarized_message_id=messages_to_summarize[-1].id,
        )

    if running_summary:
        # Only include system message if it doesn't overlap with the existing summary.
        # This is useful if the messages passed to summarize_messages already include a
        # system message with summary. This usually happens when summarization node
        # overwrites the message history.
        include_system_message = existing_system_message and not (
            existing_summary
            and existing_summary.summary in existing_system_message.content
        )

        updated_messages = cast(
            ChatPromptValue,
            final_prompt.invoke(
                {
                    "system_message": [existing_system_message]
                    if include_system_message
                    else [],
                    "summary": running_summary.summary,
                    "messages": messages[total_summarized_messages:],
                }
            ),
        )
        return SummarizationResult(
            running_summary=running_summary,
            messages=updated_messages.messages,
        )
    else:
        # no changes are needed
        return SummarizationResult(
            running_summary=None,
            messages=(
                messages
                if existing_system_message is None
                else [existing_system_message**messages]
            ),
        )


class CustomSummarizationNode(RunnableCallable):
    """
    Customized implementation of langmem.short_term.SummarizationNode.  The original
    has a bug causing the most recent user question and answer to be lost when the
    summary was updated.
    """

    def __init__(  # noqa: PLR0913
        self,
        *,
        model: LanguageModelLike,
        max_tokens: int,
        max_tokens_before_summary: int | None = None,
        max_summary_tokens: int = 256,
        token_counter: TokenCounter = count_tokens_approximately,
        initial_summary_prompt: ChatPromptTemplate = DEFAULT_INITIAL_SUMMARY_PROMPT,
        existing_summary_prompt: ChatPromptTemplate = DEFAULT_EXISTING_SUMMARY_PROMPT,
        final_prompt: ChatPromptTemplate = DEFAULT_FINAL_SUMMARY_PROMPT,
        input_messages_key: str = "messages",
        output_messages_key: str = "summarized_messages",
        name: str = "summarization",
    ) -> None:
        """
        Initialize the CustomSummarizationNode.
        """
        super().__init__(self._func, name=name, trace=False)
        self.model = model
        self.max_tokens = max_tokens
        self.max_tokens_before_summary = max_tokens_before_summary
        self.max_summary_tokens = max_summary_tokens
        self.token_counter = token_counter
        self.initial_summary_prompt = initial_summary_prompt
        self.existing_summary_prompt = existing_summary_prompt
        self.final_prompt = final_prompt
        self.input_messages_key = input_messages_key
        self.output_messages_key = output_messages_key

    def _func(self, node_input: dict[str, Any] | BaseModel) -> dict[str, Any]:
        """
        Generate a summary if needed.
        Incorporate the previous summary if present.
        Return the summary, plus most recent user input and AI response as messages
        """
        if isinstance(node_input, dict):
            messages = node_input.get(self.input_messages_key)
            context = node_input.get("context", {})
        elif isinstance(node_input, BaseModel):
            messages = getattr(node_input, self.input_messages_key, None)
            context = getattr(node_input, "context", {})
        else:
            error = f"Invalid input type: {type(node_input)}"
            raise TypeError(error)

        if messages is None:
            error = f"Missing required field `{self.input_messages_key}` in the input."
            raise ValueError(error)

        last_message = messages[-1] if messages else None
        previous_summary = context.get("running_summary")
        log.debug("Previous summary:\n\n%s\n\n", previous_summary or "N/A")
        summarization_result = summarize_messages(
            # If we are returning here from a tool call, don't include the last tool
            # message or the preceding AI message that called it.
            messages[:-2] if isinstance(last_message, ToolMessage) else messages,
            running_summary=previous_summary,
            model=self.model,
            max_tokens=self.max_tokens,
            max_tokens_before_summary=self.max_tokens_before_summary,
            max_summary_tokens=self.max_summary_tokens,
            token_counter=self.token_counter,
            initial_summary_prompt=self.initial_summary_prompt,
            existing_summary_prompt=self.existing_summary_prompt,
            final_prompt=self.final_prompt,
        )

        if (
            summarization_result.messages
            and summarization_result.messages[-1] != last_message
        ):
            # Put back the AI/Tool messages, the agent will need them
            summarization_result.messages.extend(
                messages[-2:] if isinstance(last_message, ToolMessage) else messages[-1]
            )
        state_update = {self.output_messages_key: summarization_result.messages}

        if (
            not previous_summary
            or summarization_result.running_summary.summary != previous_summary.summary
        ):
            # The running summary has changed, update the context to include
            # the latest summarization result
            log.debug("New summary:\n\n%s\n\n", summarization_result.running_summary)
            state_update["context"] = {
                **context,
                "running_summary": summarization_result.running_summary,
            }
            # If the input and output messages keys are the same, we need to remove the
            # summarized messages from the resulting message list
            if self.input_messages_key == self.output_messages_key:
                state_update[self.output_messages_key].extend(
                    [
                        RemoveMessage(id=m.id)
                        for m in messages
                        if m.id
                        in summarization_result.running_summary.summarized_message_ids
                    ],
                )

        # Insert/update the summary into the messages to be sent to the agent
        # if it isn't already there.
        output_messages = state_update[self.output_messages_key]
        if (
            summarization_result.running_summary
            and summarization_result.running_summary.summary
            and output_messages
        ):
            summary_message = [
                msg
                for msg in state_update[self.output_messages_key]
                if summarization_result.running_summary.summary in msg.content
            ]
            if not summary_message:
                log.debug("Adding summary")
                state_update[self.output_messages_key].insert(
                    1,
                    SystemMessage(
                        id=str(uuid4()),
                        content=summarization_result.running_summary.summary or "",
                    ),
                )
        log.debug(
            "\n\nMSG: %s\n\n",
            "\nMSG: ".join(
                str(m.content) for m in state_update[self.output_messages_key]
            ),
        )

        return state_update


def get_langsmith_prompt(prompt_name: str) -> str:
    """Get the text of a prompt from LangSmith by its name."""
    if settings.LANGSMITH_API_KEY:
        client = LangsmithClient(api_key=settings.LANGSMITH_API_KEY)
        try:
            prompt_template = client.pull_prompt(prompt_name)
        except Exception:
            log.exception("Error retrieving prompt '%s' from LangSmith", prompt_name)
            return None
        if prompt_template:
            return prompt_template.messages[0].prompt.template
        else:
            log.warning("Prompt '%s' not found in LangSmith.", prompt_name)
    return None


def serialize_tool_calls(tool_calls: list[dict]) -> list[dict]:
    """
    Transform LangChain tool call format to OpenAI function call format.
    :rtype: list[dict]
    """
    return [
        {
            "id": tc.get("id", ""),
            "type": tc.get("type", "tool_call"),
            "function": {
                "name": tc.get("name", ""),
                "arguments": tc.get("args", {}),
            },
        }
        for tc in tool_calls
    ]


def serialize_for_posthog(obj: Any) -> Any:  # noqa: PLR0911
    """
    Recursively serialize objects to JSON-compatible format for PostHog.
    Handles LangChain Message objects, LangGraph Send objects, and other complex types.
    """
    # Handle primitive types first
    if isinstance(obj, str | int | float | bool | type(None)):
        return obj

    # Handle BaseMessage objects from LangChain
    if hasattr(obj, "type") and hasattr(obj, "content"):
        msg_dict = {"role": obj.type, "type": obj.type, "content": obj.content}
        if hasattr(obj, "id"):
            msg_dict["id"] = obj.id
        if hasattr(obj, "additional_kwargs"):
            msg_dict["additional_kwargs"] = serialize_for_posthog(obj.additional_kwargs)
        if hasattr(obj, "tool_calls"):
            msg_dict["tool_calls"] = serialize_tool_calls(obj.tool_calls)
        return msg_dict

    # Handle Send objects from LangGraph
    if type(obj).__name__ == "Send":
        return {
            "node": obj.node if hasattr(obj, "node") else str(obj),
            "arg": serialize_for_posthog(obj.arg if hasattr(obj, "arg") else {}),
        }

    # Handle collections
    if isinstance(obj, list):
        return [serialize_for_posthog(item) for item in obj]
    if isinstance(obj, dict):
        return {k: serialize_for_posthog(v) for k, v in obj.items()}

    # Handle objects with __dict__
    if hasattr(obj, "__dict__"):
        return {k: serialize_for_posthog(v) for k, v in obj.__dict__.items()}

    # Fallback to string representation
    return str(obj)


def format_posthog_messages(messages: list[BaseMessage]) -> list[dict]:
    """
    Standardize messages to dicts.
    """
    flattened_messages = []
    for message_list in messages:
        flattened_messages.extend(message_list)
    return serialize_for_posthog(flattened_messages)


class TokenTrackingCallbackHandler(CallbackHandler):
    """
    PostHog callback handler that tracks token counts for cost
    calculation using LiteLLM's token_counter
    """

    def __init__(self, model_name: str, **kwargs):
        self.bot = kwargs.pop("bot", None)
        super().__init__(**kwargs)
        self.model_name = model_name
        self.input_tokens = 0
        self.set_trace_attributes()

    def set_trace_attributes(self):
        """Set trace attributes for PostHog"""
        self._client.capture(
            event="$ai_trace",
            distinct_id=self.bot.user_id,
            properties={
                "$ai_trace_id": self.bot.thread_id,
                "$ai_span_name": self.bot.JOB_ID,
                "botName": self.bot.JOB_ID,
            },
        )

    def on_chat_model_start(
        self,
        serialized: dict[str, Any],
        messages: list[list[BaseMessage]],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        **kwargs: Any,
    ):
        """Format messages and estimate input tokens"""
        posthog_messages = format_posthog_messages(messages)
        try:
            # Use LiteLLM token_counter with the proper format
            self.input_tokens = litellm.token_counter(
                model=self.model_name, messages=posthog_messages
            )
        except Exception:
            # Fallback to character-based estimation
            total_input_chars = 0
            for message_list in messages:
                for message in message_list:
                    if hasattr(message, "content"):
                        total_input_chars += len(str(message.content))
                    else:
                        total_input_chars += len(str(message))
            self.input_tokens = total_input_chars // 4
            log.exception("LiteLLM token_counter failed, using character estimation")
        if not hasattr(self, "_properties") or self._properties is None:
            self._properties = {}
        self._properties.update(
            {
                "question": (
                    [
                        msg["content"]
                        for msg in posthog_messages
                        if msg["type"] == "human"
                    ]
                    or [""]
                )[-1],
                "$ai_input": posthog_messages,
            }
        )

        # Call parent method
        super().on_chat_model_start(
            serialized, messages, run_id=run_id, parent_run_id=parent_run_id, **kwargs
        )

    def on_llm_end(
        self,
        response: LLMResult,
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        **kwargs: Any,
    ):
        # Calculate output tokens using LiteLLM's token_counter
        output_tokens = 0

        try:
            # Collect all output text
            output_text = (
                "".join(
                    [
                        gen_chunk.text
                        for generation in response.generations
                        for gen_chunk in generation
                        if hasattr(gen_chunk, "text") and gen_chunk.text
                    ]
                )
                if hasattr(response, "generations") and response.generations
                else ""
            )

            # Use LiteLLM token_counter for the complete output text
            if output_text:
                output_tokens = litellm.token_counter(
                    model=self.model_name, text=output_text
                )
        except Exception:
            # Fallback to character-based estimation
            if hasattr(response, "generations") and response.generations:
                for generation in response.generations:
                    for gen_chunk in generation:
                        if hasattr(gen_chunk, "text") and gen_chunk.text:
                            output_tokens += len(gen_chunk.text) // 4
            log.exception(
                "token_counter failed, using character estimation for tokens."
            )
        self._properties.update(
            {
                "answer": output_text,
                "$ai_input_tokens": self.input_tokens,
                "$ai_output_tokens": output_tokens,
                "$ai_trace_name": self.bot.JOB_ID,
                "$ai_span_name": self.bot.JOB_ID,
            }
        )
        # Call parent method
        super().on_llm_end(
            response, run_id=run_id, parent_run_id=parent_run_id, **kwargs
        )

    def _pop_run_and_capture_trace_or_span(
        self, run_id: UUID, parent_run_id: Optional[UUID], outputs: Any
    ):
        """Override to serialize outputs before passing to parent."""
        serialized_outputs = serialize_for_posthog(outputs)
        super()._pop_run_and_capture_trace_or_span(
            run_id, parent_run_id, serialized_outputs
        )


@database_sync_to_async
def query_tutorbot_output(thread_id: str) -> Optional[TutorBotOutput]:
    """Return the latest TutorBotOutput for a given thread_id"""
    return TutorBotOutput.objects.filter(thread_id=thread_id).last()


@database_sync_to_async
def create_tutorbot_output_and_checkpoints(
    thread_id: str, chat_json: Union[str, dict], edx_module_id: Optional[str]
) -> tuple[TutorBotOutput, list[DjangoCheckpoint]]:
    """Atomically create both TutorBotOutput and DjangoCheckpoint objects"""
    with transaction.atomic():
        # Get the previous TutorBotOutput to compare messages
        previous_output = (
            TutorBotOutput.objects.filter(thread_id=thread_id).order_by("-id").first()
        )
        previous_chat_json = previous_output.chat_json if previous_output else None

        # Create TutorBotOutput
        tutorbot_output = TutorBotOutput.objects.create(
            session=UserChatSession.objects.get(thread_id=thread_id),
            thread_id=thread_id,
            chat_json=chat_json,
            edx_module_id=edx_module_id or "",
        )

        checkpoints = create_tutor_checkpoints(thread_id, chat_json, previous_chat_json)

        return tutorbot_output, checkpoints


def _should_create_checkpoint(msg: dict) -> bool:
    """Determine if a message should have a checkpoint created for it."""
    # Skip ToolMessage type or tool_calls
    return not (msg.get("type") == "ToolMessage" or msg.get("tool_calls"))


def _identify_new_messages(
    filtered_messages: list[dict], previous_chat_json: Optional[Union[str, dict]]
) -> list[dict]:
    """Identify which messages are new by comparing with previous chat data."""
    if not previous_chat_json:
        return filtered_messages

    previous_chat_data = (
        json.loads(previous_chat_json)
        if isinstance(previous_chat_json, str)
        else previous_chat_json
    )
    previous_messages = previous_chat_data.get("chat_history", [])

    # Get set of existing message IDs from previous chat
    existing_message_ids = {
        msg.get("id")
        for msg in previous_messages
        if _should_create_checkpoint(msg) and msg.get("id")
    }

    # Find messages with IDs that don't exist in previous chat
    return [msg for msg in filtered_messages if msg["id"] not in existing_message_ids]


def _create_langchain_message(message: dict) -> dict:
    """Create a message in LangChain format."""
    return {
        "id": ["langchain", "schema", "messages", message["type"]],
        "lc": 1,
        "type": "constructor",
        "kwargs": {
            "id": message["id"],
            "type": message["type"].lower().replace("message", ""),
            "content": message["content"],
        },
    }


def _create_checkpoint_data(checkpoint_id: str, step: int, chat_data: dict) -> dict:
    """Create the checkpoint data structure."""
    return {
        "v": 4,
        "id": checkpoint_id,
        "ts": now_in_utc().isoformat(),
        "pending_sends": [],
        "versions_seen": {
            "__input__": {},
            "__start__": {"__start__": step + 1} if step >= 0 else {},
        },
        "channel_values": {
            "messages": [
                _create_langchain_message(msg)
                for msg in chat_data.get("chat_history", [])
            ],
            # Preserve tutor-specific data
            "intent_history": chat_data.get("intent_history"),
            "assessment_history": chat_data.get("assessment_history"),
            # Include metadata for reference
            "tutor_metadata": chat_data.get("metadata", {}),
            # Add other channel values that might be needed
            "branch:to:pre_model_hook": None,
        },
        "channel_versions": {"messages": len(chat_data.get("messages", []))},
    }


def _create_checkpoint_metadata(
    tutor_meta: dict, message: dict, step: int, thread_id: str
) -> dict:
    """Create metadata for the checkpoint based on message type."""
    writes = None
    message_type = message.get("kwargs", {}).get("type", None)
    source = "input" if message_type == HumanMessage.__name__ else "loop"
    container = WRITES_MAPPING.get(message_type, None)
    if container:
        writes = {container: {"messages": [message], **tutor_meta}}

    return {
        "step": step,
        "source": source,
        "writes": writes,
        "parents": {},
        "thread_id": thread_id,
    }


def create_tutor_checkpoints(
    thread_id: str,
    chat_json: Union[str, dict],
    previous_chat_json: Optional[Union[str, dict]] = None,
) -> list[DjangoCheckpoint]:
    """Create DjangoCheckpoint records from tutor chat data (synchronous)"""
    # Get the associated session
    try:
        session = UserChatSession.objects.get(thread_id=thread_id)
    except UserChatSession.DoesNotExist:
        return []

    # Parse and validate chat data
    chat_data = json.loads(chat_json) if isinstance(chat_json, str) else chat_json
    messages = chat_data.get("chat_history", [])

    if not messages:
        return []

    # Filter out ToolMessage types and AI messages with tool_calls
    filtered_messages = [msg for msg in messages if _should_create_checkpoint(msg)]
    if not filtered_messages:
        return []

    # Get previous checkpoint if any
    latest_checkpoint = (
        DjangoCheckpoint.objects.filter(
            thread_id=thread_id,
            checkpoint__channel_values__tutor_metadata__isnull=False,
        )
        .only("checkpoint_id")
        .order_by("-id")
        .first()
    )
    parent_checkpoint_id = (
        latest_checkpoint.checkpoint_id if latest_checkpoint else None
    )

    # Determine new messages by comparing message IDs
    new_messages = _identify_new_messages(filtered_messages, previous_chat_json)
    if not new_messages:
        return []  # No new messages to checkpoint

    # Calculate starting step based on previous checkpoint if any
    step = latest_checkpoint.metadata.get("step", -1) + 1 if latest_checkpoint else 0
    checkpoints_created = []

    # Create checkpoints only for the NEW messages

    for message in new_messages:
        checkpoint_id = str(uuid4())

        # Create checkpoint data structure
        checkpoint_data = _create_checkpoint_data(checkpoint_id, step, chat_data)

        # Create message with LangChain format and add to cumulative history
        langchain_message = _create_langchain_message(message)

        # Create metadata for this step
        metadata = _create_checkpoint_metadata(
            chat_data.get("metadata", {}), langchain_message, step, thread_id
        )

        # Create and save the checkpoint
        checkpoint, _ = DjangoCheckpoint.objects.update_or_create(
            session=session,
            thread_id=thread_id,
            checkpoint_id=checkpoint_id,
            defaults={
                "checkpoint_ns": "",
                "parent_checkpoint_id": parent_checkpoint_id,
                "type": "msgpack",
                "checkpoint": checkpoint_data,
                "metadata": metadata,
            },
        )
        parent_checkpoint_id = checkpoint_id
        checkpoints_created.append(checkpoint)
        step += 1

    return checkpoints_created
